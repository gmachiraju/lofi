{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook contains a portion of results from our NeurIPS 2023 submission. We attempt training on chunk and token embeddings and explore how these embeddings may carry explanatory signal for classification. The hypothesis is that we've encoded positional information sufficiently that we can now learn on the concatenated embeddings themselves.\n",
    "\n",
    "This notebook is specifically Step 1 in the process: gathering embeddings and creating chunk-embedded sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sequence-level Dataset Construction\n",
    "- Running inference with trained $f_\\theta$ to construct $\\mathbf{Z}$ embedded sequences, for both train and test\n",
    "- We also show a bit of the process before we dive into full-sequence training. We get an idea of what clustering over token embeddings looks like before the conversion to $\\mathbf{C}$ sequence mosaics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/envs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from embed_patches import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n",
      "GPU detected? False\n",
      "\n",
      "Note: gpu NOT available!\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)\n",
    "\n",
    "USE_GPU = True\n",
    "print(\"GPU detected?\", torch.cuda.is_available())\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "\tdevice = torch.device('cuda')\n",
    "\tprint(\"\\nNote: gpu available & selected!\")\n",
    "else:\n",
    "\tdevice = torch.device('cpu')\n",
    "\tprint(\"\\nNote: gpu NOT available!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A. Get train set stats/info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering dimensions...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "patch_dir = \"/home/data/tinycam/train/train.hdf5\"\n",
    "label_dict_path = \"/home/lofi/lofi/src/outputs/train-cam-cam16-224-background-labeldict.obj\"\n",
    "image_names = print_X_names(label_dict_path)\n",
    "train_dim_dict = gather_Z_dims(patch_dir, image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max dims (gathered from extracted patches) are: 102 108\n"
     ]
    }
   ],
   "source": [
    "i_max = 0\n",
    "j_max = 0\n",
    "for v in train_dim_dict.values():\n",
    "    if v[0] > i_max:\n",
    "        i_max = v[0]\n",
    "    if v[1] > j_max:\n",
    "        j_max = v[1]\n",
    "\n",
    "print(\"max dims (gathered from extracted patches) are:\", i_max, j_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import deserialize\n",
    "custom_train_dict_path = \"/home/data/tinycam/test/cam16-eval/my_data/cam16_train_dim_dict.obj\"\n",
    "train_dims = deserialize(custom_train_dict_path)\n",
    "\n",
    "image_names = []\n",
    "train_dim_dict = {}\n",
    "for key in train_dims.keys():\n",
    "    im_id = key.split(\".tif\")[0]\n",
    "    image_names.append(im_id)\n",
    "    train_dim_dict[im_id] = (train_dims[key][3][1], train_dims[key][3][0]) # swap dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using original image dimensions, max sizes are: 123 123\n"
     ]
    }
   ],
   "source": [
    "i_max = 0\n",
    "j_max = 0\n",
    "for v in train_dim_dict.values():\n",
    "    if v[0] > i_max:\n",
    "        i_max = v[0]\n",
    "    if v[1] > j_max:\n",
    "        j_max = v[1]\n",
    "\n",
    "print(\"Using original image dimensions, max sizes are:\", i_max, j_max)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B. Get test set stats/info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>meta1</th>\n",
       "      <th>meta2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_001</td>\n",
       "      <td>Tumor</td>\n",
       "      <td>IDC</td>\n",
       "      <td>Macro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_002</td>\n",
       "      <td>Tumor</td>\n",
       "      <td>ILC</td>\n",
       "      <td>Macro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_003</td>\n",
       "      <td>Normal</td>\n",
       "      <td>DCIS</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_004</td>\n",
       "      <td>Tumor</td>\n",
       "      <td>IDC</td>\n",
       "      <td>Micro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_005</td>\n",
       "      <td>Normal</td>\n",
       "      <td>DCIS</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   class meta1  meta2\n",
       "0  test_001   Tumor   IDC  Macro\n",
       "1  test_002   Tumor   ILC  Macro\n",
       "2  test_003  Normal  DCIS   None\n",
       "3  test_004   Tumor   IDC  Micro\n",
       "4  test_005  Normal  DCIS   None"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ref_path = \"/home/data/tinycam/test/cam16-eval/csnaftp_gdrive-16/reference.csv\"\n",
    "ref_df = pd.read_csv(ref_path, header=None, names=[\"id\", \"class\", \"meta1\", \"meta2\"])\n",
    "ref_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = [1 if el==\"Tumor\" else 0 for el in ref_df[\"class\"]]\n",
    "test_label_dict = dict(zip(ref_df[\"id\"], test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import deserialize\n",
    "custom_test_dict_path = \"/home/data/tinycam/test/cam16-eval/my_data/cam16_test_dim_dict.obj\"\n",
    "test_dims = deserialize(custom_test_dict_path)\n",
    "\n",
    "gt_path = \"/home/data/tinycam/test/cam16-eval/csnaftp_gdrive-16/lesion_annotations\"\n",
    "gt_files = os.listdir(gt_path)\n",
    "\n",
    "image_names = []\n",
    "test_dim_dict = {}\n",
    "for key in test_dims.keys():\n",
    "    im_id = key.split(\".tif\")[0]\n",
    "    image_names.append(im_id)\n",
    "    test_dim_dict[im_id] = (test_dims[key][3][1], test_dims[key][3][0]) # swap dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sizes are: 123 118\n"
     ]
    }
   ],
   "source": [
    "i_max = 0\n",
    "j_max = 0\n",
    "for v in test_dim_dict.values():\n",
    "    if v[0] > i_max:\n",
    "        i_max = v[0]\n",
    "    if v[1] > j_max:\n",
    "        j_max = v[1]\n",
    "\n",
    "print(\"max sizes are:\", i_max, j_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import serialize\n",
    "label_dict_path_test = \"/home/lofi/lofi/src/outputs/test-cam-cam16-224-background-labeldict.obj\"\n",
    "utils.serialize(test_label_dict, label_dict_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note these values above:* we want to pad our images to all be the same size for any downstream learning. Say 124 x 124 for the padding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1C. Generating Training Masks\n",
    "- relate numpy coords with salient objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cam_process import computeEvaluationMaskXML_lowres\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = \"/home/data/tinycam/test/cam16-eval/gigadb-16-17/lesion_annotations (1)\"\n",
    "gt_save_path = \"/home/data/tinycam/train/gt_masks\"\n",
    "#-------rerun if needed: can take 15-20min-----------\n",
    "# level, resolution = 5, None\n",
    "# print(\"we have\", len(os.listdir(gt_path)), \"masks to generate!\")\n",
    "# for i, mask in enumerate(os.listdir(gt_path)):\n",
    "#     id = mask.split(\".xml\")[0]\n",
    "#     print(\"started processing mask\", i, \"| ID:\", id)\n",
    "#     og_dims = (train_dims[id + \".tif\"][0][1], train_dims[id + \".tif\"][0][0]) # swap dims\n",
    "#     # og_dims = test_dims[id + \".tif\"][0]\n",
    "#     mask_np = computeEvaluationMaskXML_lowres(gt_path + \"/\" + mask, og_dims, resolution, level)\n",
    "#     if mask_np is None:\n",
    "#         break\n",
    "#     np.save(gt_save_path + \"/\" + id + \"_gt\", mask_np)\n",
    "#     print(\"finished processing mask\", i, \"| ID:\", id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a lookup dictionary of salient objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- run again if you want, 2 min------\n",
    "# gt_dict = {}\n",
    "# gt_path = \"/home/data/tinycam/train/gt_masks/\"\n",
    "# for idx, gt_file in enumerate(os.listdir(gt_path)):\n",
    "#     gt_mask = np.load(gt_path + \"/\" + gt_file)\n",
    "#     gt_id = gt_file.split(\"_gt.npy\")[0] \n",
    "#     new_dims = train_dims[gt_id + \".tif\"][3]\n",
    "#     gt_mask_sm = cv2.resize(gt_mask, (new_dims[1], new_dims[0]), interpolation=cv2.INTER_AREA)\n",
    "#     to_add = dict(((j,i), int(gt_mask_sm[i][j])) for i in range(len(gt_mask_sm)) for j in range(len(gt_mask_sm[0])))\n",
    "#     gt_dict[gt_id] = to_add\n",
    "\n",
    "# utils.serialize(gt_dict, \"outputs/train_so_dict.obj\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D. Model Inference on Train Set\n",
    "Let's now load and set up model $f_\\theta$. Choices for Camelyon16 data include:\n",
    "- `\"tile2vec\"`: an unsupervised learning model, ResNetr-16 trained from scratch\n",
    "- `\"vit_iid\"`: a (weakly) supervised learning model, ViT trained from scratch on IID fuzzy targets\n",
    "- `\"hipt\"`: a self-supervised and weakly supervised learning model, a hierarchical ViT, pre-trained and used out of the box\n",
    "- `\"plip\"`: a Foundation Model, pre-trained and used out of the box\n",
    "- `None`: skip inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model you want to run for inference\n",
    "modelstr = None #\"tile2vec\", \"plip\", \"hipt\", \"vit_iid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model selected for inference! Skipping inference...\n"
     ]
    }
   ],
   "source": [
    "if modelstr == \"tile2vec\":\n",
    "    from models import ResNet18 \n",
    "    model = ResNet18(n_classes=2, in_channels=3, z_dim=128, supervised=False, no_relu=False, loss_type='triplet', tile_size=224, activation='relu')\n",
    "    chkpt = \"/home/lofi/lofi/models/cam/to-port/ResNet18-hdf5_triplets_random_loading-224-label_selfsup-custom_loss-on_cam-cam16-filtration_background.sd\"\n",
    "    checkpoint = torch.load(chkpt, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    prev_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "elif modelstr == \"plip\":\n",
    "    from transformers import AutoProcessor, AutoTokenizer, AutoModelForZeroShotImageClassification\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinid/plip\")\n",
    "    processor = AutoProcessor.from_pretrained(\"vinid/plip\")\n",
    "    model_plip = AutoModelForZeroShotImageClassification.from_pretrained(\"vinid/plip\")\n",
    "elif modelstr == None:\n",
    "    print(\"No model selected for inference! Skipping inference...\")\n",
    "else:\n",
    "    print(\"Not yet supported for inference! Skipping inference...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run inference on the training set. We want to process all patches in the training set and then attempt to learn from the concatenated embeddings\n",
    "\n",
    "*Note:* tile2vec should take roughly 20min with 1 T4 GPU, and then closer to 60min for plip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model selected for inference! Skipping inference...\n"
     ]
    }
   ],
   "source": [
    "if modelstr == \"tile2vec\":\n",
    "    patch_dir = \"/home/data/tinycam/train/train.hdf5\"\n",
    "    save_dir = \"/home/data/tinycam/train/Zs\"\n",
    "    construct_Zs_efficient(model, patch_dir, train_dim_dict, save_dir, device, scope=\"all\")\n",
    "elif modelstr == \"plip\":\n",
    "    patch_dir = \"/home/data/tinycam/train/train.hdf5\"\n",
    "    save_dir = \"/home/data/tinycam/train/Zs_plip\"\n",
    "    construct_Zs_efficient(model_plip, patch_dir, train_dim_dict, save_dir, device, scope=\"all\", modelstr=\"plip\", processor=processor, tokenizer=tokenizer)\n",
    "elif modelstr == None:\n",
    "    print(\"No model selected for inference! Skipping inference...\")\n",
    "else:\n",
    "    print(\"Not yet supported for inference! Skipping inference...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1E. Model Inference on Test Set\n",
    "Expect 20-45min of GPU computation depending on model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model selected for inference! Skipping inference...\n"
     ]
    }
   ],
   "source": [
    "if modelstr == \"tile2vec\":\n",
    "    patch_dir = \"/home/data/tinycam/test/test.hdf5\"\n",
    "    save_dir = \"/home/data/tinycam/test/Zs\"\n",
    "    construct_Zs_efficient(model, patch_dir, test_dim_dict, save_dir, device, scope=\"all\", arm=\"test\") \n",
    "elif modelstr == \"plip\":\n",
    "    patch_dir = \"/home/data/tinycam/test/test.hdf5\"\n",
    "    save_dir = \"/home/data/tinycam/test/Zs_plip\"\n",
    "    construct_Zs_efficient(model_plip, patch_dir, test_dim_dict, save_dir, device, scope=\"all\", modelstr=\"plip\", processor=processor, tokenizer=tokenizer, arm=\"test\")\n",
    "elif modelstr == None:\n",
    "    print(\"No model selected for inference! Skipping inference...\")\n",
    "else:\n",
    "    print(\"Not yet supported for inference! Skipping inference...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now inference is complete, we can then take a look at the data sprites in `Cam-Step2-Viz.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
